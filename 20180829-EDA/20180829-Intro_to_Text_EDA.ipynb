{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# heart of DS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "# gensim for topic modeling\n",
    "from gensim import matutils\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# scikit-learn for text feature extraction & manifold learning \n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# -- visualizations --\n",
    "# for manual reading\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from wordcloud.wordcloud import WordCloud\n",
    "\n",
    "# for interactive plots\n",
    "from bokeh.io import output_notebook \n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "# for general plots\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "%matplotlib inline\n",
    "\n",
    "# for LDA visualizations\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data path\n",
    "data = ''\n",
    "\n",
    "# read data into DataFrame\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "print('Number of documents:', df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constants to remain dataset-agnostic for the remainder of the notebook\n",
    "TXT_COL = 'text'\n",
    "SNS_PALETTE = 'winter_r'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop NaNs and Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = df.shape[0]\n",
    "print('Number of docs:', n_docs)\n",
    "\n",
    "df = df.dropna(subset=[TXT_COL])\n",
    "nans_dropped = n_docs - df.shape[0]\n",
    "print('Number of NaNs removed:', nans_dropped)\n",
    "\n",
    "df = df.drop_duplicates(subset=[TXT_COL])\n",
    "dupes_dropped = n_docs - nans_dropped - df.shape[0]\n",
    "print('Number of duplicates removed:', dupes_dropped)\n",
    "print()\n",
    "print('Final number of docs:', df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index = -1\n",
    "dataset_size = df.shape[0]\n",
    "\n",
    "# text fields\n",
    "index_t = widgets.Text(description='Index:', value=str(index))\n",
    "text_ta = widgets.Textarea(description='Text:', layout=widgets.Layout(width='100%', height='200px'))\n",
    "\n",
    "# buttons\n",
    "next_b = widgets.Button(description='Next')\n",
    "\n",
    "def clear_fields():\n",
    "    text_ta.value = ''\n",
    "\n",
    "def next_doc(b):\n",
    "    global index\n",
    "    global sample_len\n",
    "    \n",
    "    # clear text fields\n",
    "    clear_fields()\n",
    "    \n",
    "    # update text fields with text from random index\n",
    "    index = np.random.randint(0, df.shape[0]-1)\n",
    "    if index < dataset_size:\n",
    "        #update text areas\n",
    "        index_t.value = str(index)\n",
    "        text_ta.value = df.iloc[index][TXT_COL]\n",
    "\n",
    "next_b.on_click(next_doc)\n",
    "\n",
    "display(index_t)\n",
    "display(text_ta)\n",
    "display(next_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform text into vectors of term frequencies\n",
    "* [CountVectorizer Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize vectorizers that will create document x term frequency matrices out of our text\n",
    "tf_unprepped_vec = CountVectorizer(ngram_range=(1,1), \n",
    "                                   token_pattern=r'(?u)\\b\\w+\\b', #words with one character or more\n",
    "                                   lowercase=False)\n",
    "\n",
    "ENGLISH_STOP_WORDS = list(set(list(ENGLISH_STOP_WORDS)+['said','got']))\n",
    "tf_vec = CountVectorizer(ngram_range=(1,1),  \n",
    "                         strip_accents='unicode', \n",
    "                         lowercase=True,\n",
    "                         token_pattern=r'(?u)\\b\\w{3,}\\b', #words with three characters or more. Be sure we're not throwing out any important words!\n",
    "                         min_df=10, # cut features that show up in at least 10 docs of the corpus\n",
    "                         max_df=0.95, # cut features that show up in the top 5% of terms by document frequency\n",
    "                         stop_words=ENGLISH_STOP_WORDS)\n",
    "\n",
    "# fit the vocabularies and return the document x term frequency matrix per vectorizer\n",
    "X_unprepped = tf_unprepped_vec.fit_transform(df[TXT_COL])\n",
    "X = tf_vec.fit_transform(df[TXT_COL])\n",
    "\n",
    "# print number of docs, size of vocabs, comparisons\n",
    "print('Number of docs:', X_unprepped.shape[0])\n",
    "print('Size of unpreprocessed vocab:', X_unprepped.shape[1])\n",
    "print('Size of preprocessed vocab:', X.shape[1])\n",
    "print()\n",
    "print('Vocab trimmed by simple preprocessing:', (X_unprepped.shape[1] - X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize docs as term-frequency feature-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf_vec.get_feature_names()\n",
    "pd.DataFrame(X.A, columns=features).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Sparsity\n",
    "* As we observe the DataFrame above, we notice many zeros for the many words in our vocabulary. This can be troublesome for modeling in terms of finding correlation between the sparse features and our desired outputs.\n",
    "* A few things that can trim or densify our feature vectors include:\n",
    "    * Further Preprocessing \n",
    "        * (e.g., stop word removal, lemma/stemming, and/or frequency filtering with min_df and max_df in the Vectorizer class)\n",
    "    * Feature Selection \n",
    "        * (e.g., [chi-squared feature selection](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html) to find the features that are more present in one class over the others)\n",
    "    * Different Feature Spaces \n",
    "        * (e.g., [word embeddings](https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285) represent words in vector lengths of 50-200. By taking the sum or average of the embeddings from a document, we are still left with a 50-200 dimension feature space instead of the above's ~13k feature dimensions)\n",
    "    \n",
    "## What do we see vs. what an ML model sees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(n=1)[TXT_COL].iloc[0].strip()\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform sample with our vectorizer\n",
    "sample_X = tf_vec.transform([sample])\n",
    "# get sample's features that have nonzero term frequencies\n",
    "nonzero_fts = tf_vec.inverse_transform(sample_X)[0]\n",
    "# get sample's nonzero term frequencies\n",
    "nonzero_tf = sample_X[sample_X.nonzero()]\n",
    "\n",
    "pd.DataFrame(nonzero_tf, columns=nonzero_fts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "* As we see in the DataFrame above, there is no order to our features. This is, yet, another thing that can be costly in modeling, especially when things such as adjectives, other descriptors, and word order play a big role in our ultimate task. Some examples:\n",
    "    * **\"this flappy bird game makes me so angry\" --> Bad Review**\n",
    "    * **\"angry birds helps me take my anger out of the day\" --> Good Review**\n",
    "* *Similar* words/features, but *different* word orders can make all the difference!\n",
    "* **Some ideas to make our life easier:**\n",
    "    * **ngrams** (We can increase the range of words to be considered as a single feature!)\n",
    "        - ngram range 2-3 for one of the above examples --> \\['this flappy', 'flappy bird', ..., 'me so angry'\\]\n",
    "        - **WARNING**: If using a range, say 1-2, the feature space will increase by **N\\*K-1** (N is size of unigram vocab, K is number of grams), so the feature engineering ideas from above should be applied when modeling comes into play.\n",
    "    * **Dependency Parse Grams** (e.g., birds_angry, birds_helps, helps_me, ...)\n",
    "    * **ngram embedding** sums/averages can come into play as well if ngram 'angry_birds' is embedded in a video game name space, or in between both good and bad review-word spaces\n",
    "    * In the realm of Deep Learning, Recurrent Neural Nets (RNNs) and Convolutional Neural Nets (CNNs) are the heavy hitters for tackling the problem of word order. \n",
    "        - [Deep Learning Methods for Text Classification](https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f)\n",
    "\n",
    "## Get Distributions\n",
    "* Are most of the documents around the same size in terms of token count? \n",
    "    * [Box-n-Whiskers](https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/box-plot-review)\n",
    "* What are the top & bottom features summed?\n",
    "* Hey, how about a word cloud?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get doc lengths\n",
    "# axis=1 means summing across the columns. [:,0] is to grab our \"total\" column, index 0, of each row\n",
    "doc_lengths = X.sum(axis=1).A[:,0] \n",
    "\n",
    "# set top_n to observe top n sum/avg features\n",
    "top_n = 25\n",
    "\n",
    "# get feature sums\n",
    "features = np.array(tf_vec.get_feature_names())\n",
    "feature_sums = X.sum(axis=0).A[0] # axis=0 means we are summing all the rows. [0] is to grab our \"total\" row\n",
    "\n",
    "# THREE things happening here: \n",
    "    # [argsort...] is to return the indices of the sorted row\n",
    "    # [::-1] reverses the sorted indices to order by DESC\n",
    "    # [:top_n] grabs a slice of the reverse sorted list of indices from beginning to n\n",
    "# get top n common features\n",
    "top_n_sum_indices = np.argsort(feature_sums)[::-1][:top_n]\n",
    "top_n_sums = feature_sums[top_n_sum_indices]\n",
    "top_n_sums_features = features[top_n_sum_indices]\n",
    "\n",
    "# get top n rare features\n",
    "bottom_n_sum_indices = np.argsort(feature_sums)[:top_n]\n",
    "bottom_n_sums = feature_sums[bottom_n_sum_indices]\n",
    "bottom_n_sums_features = features[bottom_n_sum_indices]\n",
    "\n",
    "# ---- plots ----\n",
    "\n",
    "# plot document length distributions\n",
    "fig, (ax, ax1) = plt.subplots(1,2, figsize=(14,8))\n",
    "sns.boxplot(y=doc_lengths, ax=ax)\n",
    "ax.set_title('Document Length Distributions')\n",
    "sns.boxplot(y=doc_lengths, ax=ax1)\n",
    "\n",
    "ax1.set_title('Document Length Distributions Zoomed')\n",
    "ax1.set_ylim(top=np.percentile(doc_lengths,95), bottom=-5)\n",
    "plt.show()\n",
    "\n",
    "# plot common and rare feature counts\n",
    "fig2, (ax2, ax3) = plt.subplots(1,2, figsize=(14,8))\n",
    "sns.barplot(x=top_n_sums, y=top_n_sums_features, palette=SNS_PALETTE, ax=ax2)\n",
    "ax2.set_title('%d most common features' % top_n)\n",
    "\n",
    "sns.barplot(x=bottom_n_sums, y=bottom_n_sums_features, palette=SNS_PALETTE, ax=ax3)\n",
    "ax3.set_title('%d least common features' % top_n)\n",
    "plt.show()\n",
    "\n",
    "# plot word cloud of most common features \n",
    "token_freqs = {tok:freq for tok,freq in zip(top_n_sums_features, top_n_sums)}\n",
    "wc_generator = WordCloud(background_color='whitesmoke',colormap='winter_r')\n",
    "wc = wc_generator.generate_from_frequencies(token_freqs)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.axis('off')\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do we come to any new observations?\n",
    "* **short vs. medium vs. long documents**\n",
    "    * Is some actor on some big monologue? Did someone's cat get too lonely(sitting on keyboard)? Are there multiple addendums?\n",
    "    * This could drive questions such as:\n",
    "        * \"Where does this garbage come from??\"\n",
    "        * \"How should we treat addendums? Should they be filtered out, kept, or demand our immediate attention?\" \n",
    "        * \"Should we only focus on documents of a certain size? Is there initial insight to document length as a feature for future modeling?\"\n",
    "* **common vs rare terms**\n",
    "    * Are these terms expected?\n",
    "    * Do any of the terms seem weird to exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab documents with lengths that exceed standard deviation\n",
    "doc_len_thresh = int(np.percentile(doc_lengths, 75))\n",
    "fourth_quartile_data = df[doc_lengths > doc_len_thresh]\n",
    "print('Number of documents with lengths greater than {}: {}'.format(doc_len_thresh, fourth_quartile_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = -1\n",
    "dataset_size = fourth_quartile_data.shape[0]\n",
    "\n",
    "# text fields\n",
    "index_t = widgets.Text(description='Index:', value=str(index))\n",
    "text_ta = widgets.Textarea(description='Text:', layout=widgets.Layout(width='100%', height='200px'))\n",
    "\n",
    "# buttons\n",
    "next_b = widgets.Button(description='Next')\n",
    "\n",
    "def clear_fields():\n",
    "    text_ta.value = ''\n",
    "\n",
    "def next_doc(b):\n",
    "    global index\n",
    "    global sample_len\n",
    "    \n",
    "    # clear text fields\n",
    "    clear_fields()\n",
    "    \n",
    "    # update text fields with text from random index\n",
    "    index = np.random.randint(0, fourth_quartile_data.shape[0]-1)\n",
    "    if index < dataset_size:\n",
    "        #update text areas\n",
    "        index_t.value = str(index)\n",
    "        text_ta.value = fourth_quartile_data.iloc[index][TXT_COL]\n",
    "\n",
    "next_b.on_click(next_doc)\n",
    "\n",
    "display(index_t)\n",
    "display(text_ta)\n",
    "display(next_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "### LDA Priors:\n",
    "**A lower prior _(alpha and/or beta)_ causes further sparsity in post distributions, forcing:**\n",
    "* topics to be more exclusive to documents (**alpha**)\n",
    "* words to be more exclusive to topics (**beta**)\n",
    "\n",
    "### alpha:\n",
    "**document-to-topic sparsity (\"how few topics to allow in a document\")**\n",
    "* **When observing the dataset, how many topics do you come across per document?**\n",
    "    * Is the document something like a tweet? If so, there might only be one topic discussed in that document. This would suggest a lower, sparser **alpha** to force the topic suggestion to one topic per document.\n",
    "    * Is the document something like a news article? There might be more topics discussed within. This would suggest a higher, denser **alpha** to force the topic suggestion to multiple topics per document.\n",
    "    * In Gensim's LdaModel, set parameter **alpha='auto'** to start. Observe the topic assignment to documents, check the lda_model.alpha approximations, and update accordingly.\n",
    "\n",
    "\n",
    "### beta: \n",
    "**topic-to-word sparsity (\"how few words to allow in a topic\")**\n",
    "* **When observing the dataset, how many words do you come across that you think should belong to a topic?**\n",
    "    * Is the corpus vocabulary small? If so, there might be more uniqueness in the words. This would suggest a lower, sparser **beta** to force the words to participate in less topics.\n",
    "    * Is the corpus vocabulary large? There might be more general terms that would overlap in certain topics. This would suggest a higher, denser **beta** to force the words to participate in more topics.\n",
    "    * In Gensim's LdaModel, set parameter **eta='auto'** to start. Observe the vocab overlap, check the lda_model.eta approximations, and update accordingly.\n",
    "\n",
    "**How to Choose Number of Topics?** \n",
    "* [Coherence](http://qpleple.com/topic-coherence-to-evaluate-topic-models/)\n",
    "\n",
    "**Helpful Resources**\n",
    "* [Meet the Maker: David Blei](http://videolectures.net/mlss09uk_blei_tm/)\n",
    "* [What is Topic Coherence?](https://rare-technologies.com/what-is-topic-coherence/)\n",
    "* [Gensim's LDA Model](https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "* [Gensim's Coherence Model](https://radimrehurek.com/gensim/models/coherencemodel.html)\n",
    "* [Explanation of Topic Coherence](http://qpleple.com/topic-coherence-to-evaluate-topic-models/)\n",
    "* [Step-by-step for pure Gensim Topic Modeling sans scikit-learn](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)\n",
    "* [Guided LDA](https://github.com/vi3k6i5/GuidedLDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert scipy sparse matrix to gensim sparse matrix\n",
    "if sparse.issparse(X):\n",
    "     gen_X = matutils.Sparse2Corpus(sparse=X, documents_columns=False)\n",
    "        \n",
    "# create variables that gensim requires\n",
    "word2id = tf_vec.vocabulary_\n",
    "id2word = dict((v,k) for k,v in word2id.items())\n",
    "gensim_dict = Dictionary.from_corpus(gen_X, id2word=id2word)\n",
    "\n",
    "# loop and plot coherence of lda models\n",
    "best_lda = None\n",
    "min_cvs = []\n",
    "max_cvs = []\n",
    "mean_cvs = []\n",
    "num_topics_list = list(range(2,60,8))\n",
    "for num_topics in num_topics_list:\n",
    "    t0 = time()\n",
    "    lda_model = LdaModel(corpus=gen_X, num_topics=num_topics, id2word=id2word, \n",
    "                         alpha='auto', eta='auto',  # alpha='auto', eta='auto',\n",
    "                         iterations=300, eval_every=None, random_state=13)\n",
    "    t1 = time()\n",
    "    cm = CoherenceModel(model=lda_model, corpus=gen_X, coherence='u_mass')\n",
    "    cv_per_topic = cm.get_coherence_per_topic()\n",
    "    mean_cv = np.mean(cv_per_topic)\n",
    "    mean_cvs.append(mean_cv)\n",
    "    min_cvs.append(np.min(cv_per_topic))\n",
    "    max_cvs.append(np.max(cv_per_topic))\n",
    "    print(\"Mean UMASS Coherence={} with num_topics={} in {} sec\".format(mean_cv, num_topics, (t1 - t0)))\n",
    "    if best_lda:\n",
    "        if mean_cv > best_lda['mean_cv']:\n",
    "            best_lda = {'mean_cv':mean_cv, 'cv_per_topic':cv_per_topic, 'num_topics':num_topics}\n",
    "    else:\n",
    "        best_lda = {'mean_cv':mean_cv, 'cv_per_topics':cv_per_topic, 'num_topics':num_topics}\n",
    "\n",
    "# plot CVs\n",
    "plt.figure(figsize=(10,6))\n",
    "max_df = pd.DataFrame({'Coherence':max_cvs, 'Number of Topics':num_topics_list, 'Metric':['Max Coherence']*len(max_cvs)})\n",
    "min_df = pd.DataFrame({'Coherence':min_cvs, 'Number of Topics':num_topics_list, 'Metric':['Min Coherence']*len(min_cvs)})\n",
    "mean_df = pd.DataFrame({'Coherence':mean_cvs, 'Number of Topics':num_topics_list, 'Metric':['Mean Coherence']*len(mean_cvs)})\n",
    "coherence_df = pd.concat([max_df, min_df, mean_df])\n",
    "sns.pointplot(x='Number of Topics', y='Coherence', hue='Metric', data=coherence_df, palette=SNS_PALETTE, alpha=0.5)\n",
    "plt.title('Max/Min/Mean Coherence Scores per Number of Topics')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = best_lda['num_topics']\n",
    "lda_model = LdaModel(corpus=gen_X, num_topics=num_topics, \n",
    "                     id2word=id2word, alpha='auto', eta='auto', \n",
    "                     iterations=300, eval_every=None, random_state=13)\n",
    "\n",
    "# get top topics per document\n",
    "docs_topics = lda_model.get_document_topics(gen_X)\n",
    "\n",
    "# get topics probas per word\n",
    "topics_terms = lda_model.get_topics() \n",
    "\n",
    "# print top 10 words per topic\n",
    "lda_model.print_topics(num_topics=num_topics, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(lda_model.eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize Texts and Topics into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(lda_model, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(lda_model[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prob_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                word_proba = lda_model.show_topic(topic_num)\n",
    "                topic_keywords = ', '.join([word for word, prob in word_proba])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prob_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return sent_topics_df\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(lda_model=lda_model, corpus=gen_X, texts=df[TXT_COL].values)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Topics via pyLDAvis\n",
    "**Other LDA Visualization Tools:**\n",
    "* [Stanford's Termite](http://vis.stanford.edu/papers/termite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vis = pyLDAvis.gensim.prepare(lda_model, gen_X, gensim_dict)\n",
    "lda_vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations with t-SNE\n",
    "* t-SNE reduces the dimensionality of our data while preserving the structure of local neighborhoods. \n",
    "    * This gives us an _idea_ of how our data groups together.\n",
    "* [Notes from the Maker: Laurens van der Maaten](https://lvdmaaten.github.io/tsne) **<-- Seriously, his FAQ tips are pretty helpful.**\n",
    "* [Famous Distill Article on Interpreting t-SNE](https://distill.pub/2016/misread-tsne/)\n",
    "\n",
    "**Scalable t-SNE in python:**\n",
    "* [FIt-SNE](https://github.com/KlugerLab/FIt-SNE)\n",
    "* [LargeVis](https://github.com/lferry007/LargeVis)\n",
    "\n",
    "**WARNING: t-SNE TAKES A WHILE**\n",
    "* Sample of 10000 can take from 10-20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set perplexity (nearest neighbors)\n",
    "perplexity = 30\n",
    "\n",
    "# snag a sample if conditions are crazy\n",
    "if X.shape[0] > 10000:\n",
    "    print('Dataset size is big and I don\\'t want to wait forever. Grabbing random sample.\\n')\n",
    "    sample_indices = np.random.randint(0, X.shape[0], size=10000)\n",
    "else:\n",
    "    sample_indices = np.array(range(X.shape[0]))\n",
    "\n",
    "print('Fitting T-SNE on X with {} samples and {} features.\\n'.format(sample_indices.shape[0],X.shape[1]))\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity, \n",
    "            n_iter=2000, n_iter_without_progress=200,\n",
    "            random_state=13, init='pca', method='barnes_hut')\n",
    "tsne_X = tsne.fit_transform(X[sample_indices].A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data via T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize with Bokeh\n",
    "bokeh_X = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x =       tsne_X[:,0],\n",
    "            y =       tsne_X[:,1],\n",
    "            Indices = list(range(tsne_X.shape[0])),\n",
    "            Text =    [', '.join(ft_vec) for ft_vec in \n",
    "                       tf_vec.inverse_transform(tf_vec.transform(df.iloc[sample_indices][TXT_COL]))]\n",
    "        )\n",
    "    )\n",
    "\n",
    "hover_tsne = HoverTool(names=['X'], tooltips=[('Text', '@Text'), ('Index Location','@Indices')])\n",
    "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'box_zoom', 'reset', 'save']\n",
    "plot_tsne = figure(plot_width=600, plot_height=600, tools=tools_tsne, title='t-SNE of Documents')\n",
    "plot_tsne.circle('x', 'y', size=10, fill_color='blue', \n",
    "                 alpha=0.5, line_width=0, source=bokeh_X, name='X')\n",
    "\n",
    "show(plot_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data w/ Topics via T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = df_dominant_topic.iloc[sample_indices]['Dominant_Topic'].values\n",
    "\n",
    "## visualize with Bokeh\n",
    "bokeh_X = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x =       tsne_X[:,0],\n",
    "            y =       tsne_X[:,1],\n",
    "            Indices = list(range(tsne_X.shape[0])),\n",
    "            colors =  ['#%02x%02x%02x' % (int(r),int(g),int(b)) \n",
    "                       for r,g,b,_ \n",
    "                       in 255*plt.cm.jet(topics)],\n",
    "            Text =    [', '.join(ft_vec) for ft_vec in \n",
    "                       tf_vec.inverse_transform(tf_vec.transform(df.iloc[sample_indices][TXT_COL]))]\n",
    "        )\n",
    "    )\n",
    "\n",
    "hover_tsne = HoverTool(names=['X'], tooltips=[('Text', '@Text'), ('Index Location','@Indices')])\n",
    "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'box_zoom', 'reset', 'save']\n",
    "plot_tsne = figure(plot_width=600, plot_height=600, tools=tools_tsne, title='t-SNE of Document with Topic Labels')\n",
    "plot_tsne.circle('x', 'y', size=10, fill_color='colors', \n",
    "                 alpha=0.5, line_width=0, source=bokeh_X, name='X')\n",
    "\n",
    "show(plot_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "EDA doesn't solve our problem, but serves as a wonderful set of tools to help us:\n",
    "* observe\n",
    "* question\n",
    "* research\n",
    "* hypothesize\n",
    "* experiment\n",
    "* analyze\n",
    "* conclude about the value of the data at hand.\n",
    "\n",
    "It helps us become familiar with the data, and helps us become confident about next steps. \n",
    "\n",
    "And if the conclusion is to move towards modeling the data, we'll be ready!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
